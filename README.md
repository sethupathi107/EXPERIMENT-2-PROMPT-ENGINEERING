# EXP-2-PROMPT-ENGINEERING-
### Name: Sethupathi K
### Reg no: 212223040189
## Aim: 
Comparative Analysis of different types of Prompting patterns and explain with Various Test Scenarios

Experiment:
Test and compare how different pattern models respond to various prompts (broad or unstructured) versus basic prompts (clearer and more refined) across multiple scenarios. 
Analyze the quality, accuracy, and depth of the generated responses.


## Algorithm:

**Step 1: Identify the Task**

* Determine whether the task is translation, reasoning, summarization, creative writing, question answering, or role-based.

**Step 2: Select the Prompting Style**

1. **Zero-Shot Prompting**

   * Directly ask the model without examples.
   * Example: “Translate the following sentence into French: I love learning AI.”

2. **Few-Shot Prompting**

   * Provide a few examples to set context.
   * Example: “English: Hello → French: Bonjour. English: Thank you → French: Merci. English: I love learning AI → French: …”

3. **Chain-of-Thought (CoT) Prompting**

   * Encourage the model to reason step by step.
   * Example: “Solve this math problem step by step: If a pen costs ₹10 and a book costs ₹50, what is the total cost of 2 pens and 3 books?”

4. **Instruction-Based Prompting**

   * Give explicit and clear task-oriented instructions.
   * Example: “Summarize this article in 3 bullet points, highlighting key arguments and examples.”

5. **Role-Based Prompting**

   * Assign a role/persona to the model.
   * Example: “You are a financial advisor. Suggest a diversified portfolio for a 25-year-old investor with moderate risk tolerance.”

6. **Broad/Unstructured Prompting**

   * Use vague or incomplete instructions.
   * Example: “Tell me something about finance.”

**Step 3: Generate Response**

* Provide the chosen prompt to the LLM.
* Collect the output generated by the model.

**Step 4: Evaluate the Response**

* Assess the output for accuracy, clarity, creativity, and depth.

**Step 5: Compare Across Styles**

* Run the same task with different prompting styles.
* Compare strengths, weaknesses, and best-fit scenarios.

## Output

[Sethupathi_prompt1.pdf](https://github.com/user-attachments/files/23897968/sri_prompt1.pdf)


## Result
This experiment demonstrates that the way a prompt is structured strongly impacts the output of Generative AI systems. Broad prompts → produce vague, shallow, and sometimes irrelevant responses. Instructional, Role-based, and CoT prompts → significantly improve accuracy, depth, and clarity. Few-shot prompting adds adaptability, while zero-shot is useful for quick queries.

## Final Insight:
Prompt engineering is a critical skill. For high-quality results, prompts should be clear, contextual, and structured, often combining instructional + role-based + CoT styles for maximum effectiveness.
